{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe8edac-5020-4ac0-9213-6e3216bef324",
   "metadata": {},
   "source": [
    "# Analyzing Cain's Jawbone with Word Vectoring and K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c4aa2-b162-414d-9290-4b7295015827",
   "metadata": {},
   "source": [
    "## Cain’s Jawbone\n",
    "\n",
    "[We suggested a solution to Cain’s Jawbone as a personal project almost a year ago.](https://glthr.com/cj)\n",
    "\n",
    "Cain’s Jawbone, by Edward Powys Mathers, is a puzzle consisting of 100 shuffled pages. To manually solve it, the following heuristic was used:\n",
    "\n",
    "* Group the pages by narrators (there are eight of them, concealed in the narration).\n",
    "* Then, sort the pages in each group.\n",
    "* Last but not least, sort the groups themselves.\n",
    "\n",
    "Now, we would like to investigate whether artificial intelligence (in a broad sense) could be used to find a solution more efficiently, at least partially. We are only at the very beginning of this analysis.\n",
    "\n",
    "First, let's load *Cain’s Jawbone* as a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91644c3f-2270-44b7-8d97-fe08a6b012c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I sit down alone at the appointed table and ta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I plunged for the last time. The few remaining...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>At my meeting with Clement yesterday, he had b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>And I really think I would have preferred the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I hated my eye for being caught by what didn’t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   page                                               text\n",
       "0     1  I sit down alone at the appointed table and ta...\n",
       "1     2  I plunged for the last time. The few remaining...\n",
       "2     3  At my meeting with Clement yesterday, he had b...\n",
       "3     4  And I really think I would have preferred the ...\n",
       "4     5  I hated my eye for being caught by what didn’t..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"./data/jawbone.json\")\n",
    "\n",
    "# ensure that the input file is complete\n",
    "if len(df[df[\"text\"].notna() & (df[\"text\"] != \"\")]) != 100:\n",
    "    print(\"Error: the original corpus is incomplete\")\n",
    "    exit\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02cfaa0-0dda-43d5-a45b-d6cf9121f13f",
   "metadata": {},
   "source": [
    "_(Reserved for later use)_ Then, an unofficial solution from our previous manual attempt shared at [https://glthr.com/cj/](glthr.com/cj/) is loaded. It will only adjust the models as the training is unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65f0688-7b41-404b-9ef5-679135b3acb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Oscar Mills': [92, 42, 2, 38, 90, 62, 53, 14, 66, 74, 83, 31],\n",
       " 'May Doncaster': [72, 49, 13, 26, 44, 94, 16, 68, 58, 5, 81, 77, 35],\n",
       " 'Sir Paul Trinder': [28, 9, 52, 12, 50, 65, 18, 34, 76, 89, 78, 98],\n",
       " 'Clement DeAth': [54, 80, 91, 71, 59, 86, 24, 6, 21, 36, 29, 75, 60],\n",
       " 'Bill Hardy': [1, 96, 17, 23, 87, 43, 84, 63, 47, 4, 30, 100],\n",
       " 'Jasmine Gay': [69, 56, 27, 37, 7, 45, 82, 41, 93, 11, 15, 95, 70],\n",
       " 'John Walker': [32, 3, 25, 73, 67, 51, 55, 61, 20, 40, 97, 85],\n",
       " 'Henry (dog)': [64, 22, 79, 48, 88, 39, 46, 10, 99, 8, 19, 57, 33]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"./data/unofficial_solution.json\")\n",
    "possible_solutions = json.load(f)\n",
    "f.close()\n",
    "\n",
    "possible_solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df017433-1ad0-4661-8a21-307539bda1e5",
   "metadata": {},
   "source": [
    "## Naive Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01801c6f-0a74-4fdc-9a00-e8ce6f89dd25",
   "metadata": {},
   "source": [
    "To begin our analysis of the suitability of artificial intelligence, we perform a classic unsupervised learning approach (word vectorization and k-means clustering) to group the pages automatically. At this stage of the analysis, we explicitly set `k` to the expected number of groups (8). \n",
    "\n",
    "### TF-IDF word vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a7cec00-a8d6-4ec8-814e-ebc4eab4d313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3786) vectors\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_sentences = [word_tokenize(sentence) for sentence in df[\"text\"]]\n",
    "\n",
    "# words vectorization (TF-IDF)\n",
    "vectorizer = TfidfVectorizer(analyzer=\"word\")\n",
    "X = vectorizer.fit_transform([\" \".join(tokens) for tokens in tokenized_sentences])\n",
    "\n",
    "print(f\"{X.shape} vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4224e5-8356-4a3e-8656-e96a57a76383",
   "metadata": {},
   "source": [
    "### K-means clustering performed with the *known* number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f7a2180-3f70-445a-a601-3ef65e3f5907",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m\n\u001b[1;32m      8\u001b[0m kmeans \u001b[38;5;241m=\u001b[39m KMeans(n_clusters\u001b[38;5;241m=\u001b[39mk, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m kmeans\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mlabels_\n\u001b[1;32m     12\u001b[0m centers \u001b[38;5;241m=\u001b[39m kmeans\u001b[38;5;241m.\u001b[39mcluster_centers_\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:1530\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1527\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitialization complete\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;66;03m# run a k-means once\u001b[39;00m\n\u001b[0;32m-> 1530\u001b[0m labels, inertia, centers, n_iter_ \u001b[38;5;241m=\u001b[39m kmeans_single(\n\u001b[1;32m   1531\u001b[0m     X,\n\u001b[1;32m   1532\u001b[0m     sample_weight,\n\u001b[1;32m   1533\u001b[0m     centers_init,\n\u001b[1;32m   1534\u001b[0m     max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iter,\n\u001b[1;32m   1535\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m   1536\u001b[0m     tol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tol,\n\u001b[1;32m   1537\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_threads,\n\u001b[1;32m   1538\u001b[0m )\n\u001b[1;32m   1540\u001b[0m \u001b[38;5;66;03m# determine if these results are the best so far\u001b[39;00m\n\u001b[1;32m   1541\u001b[0m \u001b[38;5;66;03m# we chose a new run if it has a better inertia and the clustering is\u001b[39;00m\n\u001b[1;32m   1542\u001b[0m \u001b[38;5;66;03m# different from the best so far (it's possible that the inertia is\u001b[39;00m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;66;03m# slightly better even if the clustering is the same with potentially\u001b[39;00m\n\u001b[1;32m   1544\u001b[0m \u001b[38;5;66;03m# permuted labels, due to rounding errors)\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_inertia \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1546\u001b[0m     inertia \u001b[38;5;241m<\u001b[39m best_inertia\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_same_clustering(labels, best_labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_clusters)\n\u001b[1;32m   1548\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:688\u001b[0m, in \u001b[0;36m_kmeans_single_lloyd\u001b[0;34m(X, sample_weight, centers_init, max_iter, verbose, tol, n_threads)\u001b[0m\n\u001b[1;32m    684\u001b[0m strict_convergence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    686\u001b[0m \u001b[38;5;66;03m# Threadpoolctl context to limit the number of threads in second level of\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;66;03m# nested parallelism (i.e. BLAS) to avoid oversubscription.\u001b[39;00m\n\u001b[0;32m--> 688\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m threadpool_limits(limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblas\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m    690\u001b[0m         lloyd_iter(\n\u001b[1;32m    691\u001b[0m             X,\n\u001b[1;32m    692\u001b[0m             sample_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m             n_threads,\n\u001b[1;32m    699\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/fixes.py:74\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[0;34m(limits, user_api)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m controller\u001b[38;5;241m.\u001b[39mlimit(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m threadpoolctl\u001b[38;5;241m.\u001b[39mthreadpool_limits(limits\u001b[38;5;241m=\u001b[39mlimits, user_api\u001b[38;5;241m=\u001b[39muser_api)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[0;34m(self, limits, user_api)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, limits\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params(limits, user_api)\n\u001b[0;32m--> 171\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_threadpool_limits()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m modules \u001b[38;5;241m=\u001b[39m _ThreadpoolInfo(prefixes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prefixes,\n\u001b[1;32m    269\u001b[0m                           user_api\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_user_api)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;66;03m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;66;03m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;66;03m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module\u001b[38;5;241m.\u001b[39mprefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[0;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m user_api \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m user_api\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 340\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_modules()\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:371\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Loop through loaded libraries and store supported ones\"\"\"\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdarwin\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_dyld()\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    373\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_modules_with_enum_process_module_ex()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:428\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_dyld\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    425\u001b[0m filepath \u001b[38;5;241m=\u001b[39m filepath\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;66;03m# Store the module if it is supported and selected\u001b[39;00m\n\u001b[0;32m--> 428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_module_from_path(filepath)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:515\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._make_module_from_path\u001b[0;34m(self, filepath)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefixes \u001b[38;5;129;01mor\u001b[39;00m user_api \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_api:\n\u001b[1;32m    514\u001b[0m     module_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mglobals\u001b[39m()[module_class]\n\u001b[0;32m--> 515\u001b[0m     module \u001b[38;5;241m=\u001b[39m module_class(filepath, prefix, user_api, internal_api)\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mappend(module)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:606\u001b[0m, in \u001b[0;36m_Module.__init__\u001b[0;34m(self, filepath, prefix, user_api, internal_api)\u001b[0m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minternal_api \u001b[38;5;241m=\u001b[39m internal_api\n\u001b[1;32m    605\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mCDLL(filepath, mode\u001b[38;5;241m=\u001b[39m_RTLD_NOLOAD)\n\u001b[0;32m--> 606\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mversion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_version()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_threads()\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_extra_info()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/threadpoolctl.py:646\u001b[0m, in \u001b[0;36m_OpenBLASModule.get_version\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m get_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynlib, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenblas_get_config\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    644\u001b[0m                      \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    645\u001b[0m get_config\u001b[38;5;241m.\u001b[39mrestype \u001b[38;5;241m=\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mc_char_p\n\u001b[0;32m--> 646\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenBLAS\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m config[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# the number of clusters is defined from the unofficial solution\n",
    "k = 8\n",
    "\n",
    "kmeans = KMeans(n_clusters=k, random_state=1)\n",
    "kmeans.fit(X)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# dimensionality reduction (2 dimensions)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X.toarray())\n",
    "\n",
    "# plot\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap=\"viridis\", edgecolor=\"k\")\n",
    "for i, txt in enumerate(df[\"page\"]):\n",
    "    plt.annotate(\n",
    "        txt,\n",
    "        (X_pca[i, 0], X_pca[i, 1]),\n",
    "        textcoords=\"offset points\",\n",
    "        xytext=(0, 5),\n",
    "        ha=\"center\",\n",
    "    )\n",
    "\n",
    "plt.title(\"Cain’s Jawbone — K-Means — 2D PCA Reduction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb7eb09-8a6f-427d-a4de-1ecda273ecd1",
   "metadata": {},
   "source": [
    "## Preliminary Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e74bf13-809e-4c8a-94e6-23322fad47cb",
   "metadata": {},
   "source": [
    "We anticipated that the result would be limited because of (a) the extratextual clues, (b) the simplicity of our approach—at this stage, at least—, and (c) the fact that the pages were all written by the same person pretending to be eight different narrators (meaning that the style difference between the pages is artificial, not intrinsic). \n",
    "\n",
    "While the clusters do not greatly overlap the expected groups (even though k is set to the actual number of narrators), the representation of the PCA reduction of naive k-means clustering revealed an “affinity” (which remains to be formalized) between some of the pages which is more or less in line with the expected solution. This is particularly true for Bill Hardy and Henry the dog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db4de99-9f11-4514-9b77-80ce950da55c",
   "metadata": {},
   "source": [
    "![Clusters](./clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace3bcb2-623d-49a1-98c1-3dc6eebb5484",
   "metadata": {},
   "source": [
    "These preliminary results tend to indicate that NLP may be suitable, to some extent, to identify the narrators. To that end, it will need to be improved to use additional clues and automatically determine the number of narrators (we tried to use the elbow method to that end, but as it was inconclusive in the context of this preliminary analysis, and therefore had to specify this number ourselves)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202c5ba4",
   "metadata": {},
   "source": [
    "These results prompt us to attempt a more comprehensive approach: [classify the pages with BERT](./2_BERT.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
